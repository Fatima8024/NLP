{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1nao_5_mQQs9ePyR8nymVu8XhVPUJhFBe",
      "authorship_tag": "ABX9TyN1nlo9Vh2Fhn7BtuXilpo+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fatima8024/NLP/blob/main/NLP_Similar_questions_Quora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Restart runtime"
      ],
      "metadata": {
        "id": "5YHEFOREx925"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o7yXCVExK7RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y gensim numpy scipy\n",
        "!pip install --no-cache-dir --force-reinstall \\\n",
        "    \"numpy>=1.26.0,<2.0.0\" \\\n",
        "    \"scipy>=1.7.0,<1.14.0\" \\\n",
        "    \"gensim==4.3.3\"\n"
      ],
      "metadata": {
        "id": "Z3aGhSRPK7x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g527Z9HfWkGe"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------\n",
        "# Step 1: Import required libraries\n",
        "# ----------------------------------------\n",
        "\n",
        "import pandas as pd   #For Reading CSV files\n",
        "import numpy as np    #For handling word vectors\n",
        "import string         #For Removing punctuation during token cleaning\n",
        "\n",
        "import nltk           #Tokenizing text and removing stopwords\n",
        "from nltk.corpus import stopwords    #Removes common filler words\n",
        "from nltk.tokenize import word_tokenize   #Breaks text into words\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # To convert text into vectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity   #Computes similarity between two vectors\n",
        "\n",
        "#from gensim.models import Word2Vec    # For averaging word vectors to represent questions\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sentence_transformers import SentenceTransformer  #Converting entire sentences into dense vectors that capture meaning\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 2: Load Dataset\n",
        "# ====================================================\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define the file path (update if needed)\n",
        "file_path = '/content/drive/MyDrive/NLP Project/questions.csv'\n",
        "\n",
        "# Step 3: Read the CSV file\n",
        "#import pandas as pd\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 4: Confirm it's loaded\n",
        "print(\"✅ Dataset loaded successfully!\")\n",
        "print(df.head())\n",
        "print(\"Total question pairs processed:\", len(df))\n"
      ],
      "metadata": {
        "id": "EhDsGNqEWumP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 3: Data Preprocessing\n",
        "# Clean, lowercase, remove stopwords/punctuation, tokenize\n",
        "# ====================================================\n",
        "stop_words = set(stopwords.words('english'))     #Loads the stopping words\n",
        "\n",
        "def preprocess(text):\n",
        "    text = str(text).lower()     #Converts text to lowercase\n",
        "    tokens = word_tokenize(text)   #Breaks text into list of individual words or \"tokens\"\n",
        "    return [w for w in tokens if w not in stop_words and w not in string.punctuation]\n",
        "\n",
        "# 🔍 Debug: test the preprocess function on the first row\n",
        "print(\"🔍 Sample tokenization:\", preprocess(df['question1'].iloc[0]))\n",
        "\n",
        "# ✅ Apply preprocessing to both questions\n",
        "df['q1_tokens'] = df['question1'].apply(preprocess)  #Runs preprocess() on every row in the question1 and question2 columns.\n",
        "df['q2_tokens'] = df['question2'].apply(preprocess)  #Two new columns q1_tokens, q2_tokens, each holding a list of cleaned tokens\n",
        "                                                     #are formed.\n",
        "\n",
        "# Rejoin tokens for methods that require full sentences\n",
        "df['q1_clean'] = df['q1_tokens'].apply(lambda x: \" \".join(x)) #Converts the token lists back into plain text strings.\n",
        "df['q2_clean'] = df['q2_tokens'].apply(lambda x: \" \".join(x)) #As TF-IDF or Sentence-BERT expect full sentence strings, not token lists.\n",
        "\n",
        " #🔍 Confirm columns are created\n",
        "print(df[['q1_clean', 'q2_clean']].head())\n",
        "\n",
        "print(\"✅ Finished tokenization\")    #Text preprocessing ensures that all models — TF-IDF, Word2Vec, SBERT — work on cleaned and normalized data"
      ],
      "metadata": {
        "id": "4cDRXTWwW6L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA: Distribution and Repeated Questions\n",
        "\n",
        "# Distribution of duplicate and non-duplicate questions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"🔢 Label Distribution:\")\n",
        "print(df['is_duplicate'].value_counts())  # 0: not duplicate, 1: duplicate\n",
        "print(\"\\n📊 Percentage Distribution:\")\n",
        "print((df['is_duplicate'].value_counts() / df['is_duplicate'].count()) * 100)\n",
        "df['is_duplicate'].value_counts().plot(kind='bar', title='Class Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Repeated questions\n",
        "print(\"\\n🔁 Repeated Questions Stats:\")\n",
        "qid = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n",
        "print('Total Unique Questions:', np.unique(qid).shape[0])\n",
        "x = qid.value_counts() > 1\n",
        "print('Questions Appearing More Than Once:', x[x].shape[0])"
      ],
      "metadata": {
        "id": "hUVd30GrJj0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 4: Method 1 – TF-IDF + Cosine Similarity\n",
        "# Convert text to TF-IDF vectors and compute similarity\n",
        "# and detect if two questions are duplicates.\n",
        "# ====================================================\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "tfidf = TfidfVectorizer()    #Converts text into vectors for similarity calculation\n",
        "tfidf.fit(pd.concat([df['q1_clean'], df['q2_clean']]))   #Learns full vocabulary before transforming\n",
        "\n",
        "q1_vec = tfidf.transform(df['q1_clean'])   #Converts cleaned text into sparse TF-IDF vectors\n",
        "q2_vec = tfidf.transform(df['q2_clean'])\n",
        "\n",
        "# Calculate cosine similarity\n",
        "df['cosine_sim'] = [cosine_similarity(q1, q2)[0][0] for q1, q2 in zip(q1_vec, q2_vec)]   #Stores the similarity score (0–1) in a new column\n",
        "X_tfidf = np.array(df['cosine_sim'].tolist()).reshape(-1, 1)\n",
        "y = df['is_duplicate'].values\n",
        "\n",
        "\n",
        "# Classify based on similarity threshold\n",
        "threshold = 0.7\n",
        "df['tfidf_pred'] = (df['cosine_sim'] >= threshold).astype(int)   #If similarity is above 0.7, classify as a duplicate (1), else not (0)\n",
        "\n",
        "print(\"✅ TF-IDF complete\")\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "  #Imports accuracy, F1, and confusion matrix for each model\n",
        "\n",
        "print(\"\\n🔹 TF-IDF + Cosine Similarity\")\n",
        "print(\"Accuracy:\", accuracy_score(df['is_duplicate'], df['tfidf_pred']))\n",
        "print(classification_report(df['is_duplicate'], df['tfidf_pred']))\n",
        "print(confusion_matrix(df['is_duplicate'], df['tfidf_pred']))"
      ],
      "metadata": {
        "id": "VQxa_h6uXBpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 5: Method 2 – Word2Vec + Logistic Regression\n",
        "# Use word embeddings and a classifier\n",
        "# ====================================================\n",
        "\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Step1: Train Word2Vec model\n",
        "sentences = df['q1_tokens'].tolist() + df['q2_tokens'].tolist()   #Combines all tokenized question pairs into one list of token lists\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2)    #Trains word embeddings\n",
        "print(\"✅ Word2Vec training complete\")\n",
        "\n",
        "# Step 2: Create averaged vectors for each question\n",
        "def avg_vector(tokens, model, dim=100):\n",
        "    valid_words = [w for w in tokens if w in model.wv]\n",
        "    return np.mean(model.wv[valid_words], axis=0) if valid_words else np.zeros(dim)\n",
        "\n",
        "# Use apply instead of extra function\n",
        "df['q1_vec'] = df['q1_tokens'].apply(lambda x: avg_vector(x, w2v_model))\n",
        "df['q2_vec'] = df['q2_tokens'].apply(lambda x: avg_vector(x, w2v_model))\n",
        "\n",
        "# Step 3: Create feature vectors using absolute difference\n",
        "X_w2v = np.array([np.abs(q1 - q2) for q1, q2 in zip(df['q1_vec'], df['q2_vec'])])\n",
        "y = df['is_duplicate'].values\n",
        "\n",
        "# Step 4: Train/test split and model training\n",
        "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(X_w2v, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_w2v = LogisticRegression(max_iter=1000)\n",
        "model_w2v.fit(X_train_w2v, y_train)\n",
        "y_pred = model_w2v.predict(X_test_w2v)\n",
        "\n",
        "# Step 5: Evaluation\n",
        "print(\"\\n🔹 Word2Vec + Logistic Regression\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "98tMojM3XHvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 6: Method 3 – Sentence-BERT + Cosine Similarity\n",
        "# Use transformer embeddings for semantic similarity\n",
        "# ====================================================\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer  # to load SBERT model\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # to compute similarity between sentence vectors\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # for evaluation\n",
        "\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate sentence embeddings\n",
        "q1_embed = sbert_model.encode(df['q1_clean'].tolist(), show_progress_bar=True)\n",
        "q2_embed = sbert_model.encode(df['q2_clean'].tolist(), show_progress_bar=True)\n",
        "\n",
        "print(\"✅ Sentence-BERT complete\")\n",
        "\n",
        "# Cosine similarity\n",
        "df['sbert_sim'] = [cosine_similarity([q1], [q2])[0][0] for q1, q2 in zip(q1_embed, q2_embed)]\n",
        "\n",
        "# Classify using threshold\n",
        "df['sbert_pred'] = (df['sbert_sim'] >= 0.7).astype(int)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n🔹 Sentence-BERT + Cosine Similarity\")\n",
        "print(\"Accuracy:\", accuracy_score(df['is_duplicate'], df['sbert_pred']))\n",
        "print(classification_report(df['is_duplicate'], df['sbert_pred']))\n",
        "print(confusion_matrix(df['is_duplicate'], df['sbert_pred']))\n"
      ],
      "metadata": {
        "id": "NqIr_sjJXT0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF + Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_tf, X_test_tf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "model_tfidf = LogisticRegression()\n",
        "model_tfidf.fit(X_train_tf, y_train)\n",
        "\n",
        "# Word2Vec + Logistic Regression\n",
        "X_train_w2v, X_test_w2v, _, _ = train_test_split(X_w2v, y, test_size=0.2, random_state=42)\n",
        "model_w2v = LogisticRegression(max_iter=200)\n",
        "model_w2v.fit(X_train_w2v, y_train)\n"
      ],
      "metadata": {
        "id": "v5XdFuJwcKiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save trained models\n",
        "joblib.dump(model_tfidf, \"model_tfidf.pkl\")\n",
        "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
        "joblib.dump(model_w2v, \"model_w2v.pkl\")\n",
        "w2v_model.save(\"word2vec.model\")  # Save Word2Vec separately\n"
      ],
      "metadata": {
        "id": "aK_nMc5zcQLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 7: Final Accuracy Comparison\n",
        "# ====================================================\n",
        "print(\"\\n📊 Final Accuracy Comparison:\")\n",
        "print(\"TF-IDF      :\", accuracy_score(df['is_duplicate'], df['tfidf_pred']))\n",
        "print(\"Word2Vec    :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Sentence-BERT:\", accuracy_score(df['is_duplicate'], df['sbert_pred']))\n"
      ],
      "metadata": {
        "id": "SuPcUt9JXdIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 8: Import Libraries for Accuracy Comparison\n",
        "# ====================================================\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
      ],
      "metadata": {
        "id": "wjCkxRRky6Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 9: Create Comparison Table (Accuracy + F1 Score)\n",
        "\n",
        "# ====================================================\n",
        "\n",
        "print(df.columns)\n",
        "\n",
        "# Collect metrics\n",
        "methods = ['TF-IDF + Cosine', 'Word2Vec + Logistic Regression', 'Sentence-BERT + Cosine']\n",
        "accuracies = [\n",
        "    accuracy_score(df['is_duplicate'], df['tfidf_pred']),\n",
        "    accuracy_score(y_test, y_pred),\n",
        "    accuracy_score(df['is_duplicate'], df['sbert_pred'])\n",
        "]\n",
        "\n",
        "# Get F1-scores\n",
        "f1_scores = [\n",
        "    classification_report(df['is_duplicate'], df['tfidf_pred'], output_dict=True)['weighted avg']['f1-score'],\n",
        "    classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score'],\n",
        "    classification_report(df['is_duplicate'], df['sbert_pred'], output_dict=True)['weighted avg']['f1-score']\n",
        "]\n",
        "\n",
        "# Build DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Method': methods,\n",
        "    'Accuracy': accuracies,\n",
        "    'F1 Score': f1_scores\n",
        "})\n",
        "\n",
        "\n",
        "# Show results\n",
        "print(\"🔍 Final Model Comparison:\")\n",
        "display(results_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "dqbwgYQgy583"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 10: Plot Bar Chart\n",
        "# ====================================================\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "bar_width = 0.35\n",
        "index = range(len(methods))\n",
        "\n",
        "plt.bar(index, accuracies, bar_width, label='Accuracy')\n",
        "plt.bar([i + bar_width for i in index], f1_scores, bar_width, label='F1 Score')\n",
        "\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks([i + bar_width / 2 for i in index], methods, rotation=15)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uOl1jZODzNXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***What the Results Show***"
      ],
      "metadata": {
        "id": "E5sMBCJlG10D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model                      | Accuracy | F1 Score | Summary                                                 |\n",
        "| -------------------------- | -------- | -------- | ------------------------------------------------------- |\n",
        "| **TF-IDF + Cosine**        | \\~0.66   | \\~0.65   | Performs moderately well; slightly balanced             |\n",
        "| **Word2Vec + Logistic**    | \\~0.64   | \\~0.60   | Lower F1 score → more false positives or negatives      |\n",
        "| **Sentence-BERT + Cosine** | \\~0.72   | \\~0.72   | Best overall performance — both in accuracy and balance |\n"
      ],
      "metadata": {
        "id": "GqMsketrGuPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅** 1. Sentence-BERT is the best model**\n",
        "Highest accuracy and F1 score\n",
        "\n",
        "Captures semantic meaning better than TF-IDF or Word2Vec\n",
        "\n",
        "Ideal for duplicate detection where meaning matters more than wording\n",
        "\n",
        "⚠️ **2. Word2Vec performs worse than expected**\n",
        "Accuracy is decent, but F1 score drops, suggesting it may:\n",
        "\n",
        "Predict one class more than the other\n",
        "\n",
        "Be less effective on shorter/rare-word questions\n",
        "\n",
        "Could be improved with better tuning or more training data\n",
        "\n",
        "⚖️** 3. TF-IDF is surprisingly competitive**\n",
        "Simpler, faster, and performs decently\n",
        "\n",
        "May be a good lightweight baseline when resources are limited"
      ],
      "metadata": {
        "id": "qqR6wWq_HLs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 11: Show Confusion Matrices\n",
        "# ====================================================\n",
        "\n",
        "# Confusion matrices\n",
        "cm_tfidf = confusion_matrix(df['is_duplicate'], df['tfidf_pred'])\n",
        "cm_w2v   = confusion_matrix(y_test, y_pred)\n",
        "cm_sbert = confusion_matrix(df['is_duplicate'], df['sbert_pred'])\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "for ax, cm, title in zip(axes,\n",
        "                         [cm_tfidf, cm_w2v, cm_sbert],\n",
        "                         ['TF-IDF', 'Word2Vec', 'Sentence-BERT']):\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "    ax.set_title(f'{title} Confusion Matrix')\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YnrJKlHBzM1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Confusion Matrix Terms **\n",
        "Each matrix compares:\n",
        "\n",
        "Actual labels (rows) vs Predicted labels (columns)\n",
        "\n",
        "Predicted: 0\tPredicted: 1\n",
        "Actual: 0\tTN (True Negatives)\tFP (False Positives)\n",
        "\n",
        "Actual: 1\tFN (False Negatives)\tTP (True Positives)"
      ],
      "metadata": {
        "id": "M0BIWDrVKUMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF + Cosine Similarity\n",
        "Many false negatives (83k): TF-IDF fails to catch many actual duplicates.\n",
        "\n",
        "Decent TN and TP: Predicts non-duplicates well.\n",
        "\n",
        "🔻 Weak at semantic similarity: Since it’s based on exact word overlap."
      ],
      "metadata": {
        "id": "aC92gezhKlZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔹 Step 12: Show Sample Similar Question Predictions\n",
        "# ====================================================\n",
        "\n",
        "\n",
        "# Number of examples to show\n",
        "n = 5\n",
        "\n",
        "# Sample duplicates detected by TF-IDF model\n",
        "print(\"🔍 TF-IDF Predictions (Detected as Duplicates):\")\n",
        "sample_tfidf = df[(df['tfidf_pred'] == 1)].sample(n)\n",
        "for i, row in sample_tfidf.iterrows():\n",
        "    print(f\"\\nQ1: {row['question1']}\")\n",
        "    print(f\"Q2: {row['question2']}\")\n",
        "    print(f\"Cosine Similarity: {row['cosine_sim']:.3f}\")\n",
        "\n",
        "# Create a DataFrame with the indices from the original df\n",
        "df_reset = df.reset_index(drop=True)\n",
        "df_test = pd.DataFrame(X_test)\n",
        "df_test['pred'] = y_pred\n",
        "\n",
        "# Sample 5 predicted duplicates\n",
        "sample_idx = df_test[df_test['pred'] == 1].sample(n).index\n",
        "\n",
        "\n",
        "# Sample duplicates detected by Word2Vec model\n",
        "print(\"\\n\\n🔍 Word2Vec + Logistic Regression Predictions (Detected as Duplicates):\")\n",
        "# We'll use the same X_test/y_pred from earlier\n",
        "import numpy as np\n",
        "df_test = pd.DataFrame(X_test, columns=[f'feat_{i}' for i in range(X_test.shape[1])])\n",
        "df_test['pred'] = y_pred\n",
        "df_test['index'] = y_test.tolist()\n",
        "sample_idx = df_test[df_test['pred'] == 1].sample(n).index\n",
        "\n",
        "for i in sample_idx:\n",
        "    print(f\"\\nQ1: {df_reset.iloc[X_train.shape[0]+i]['question1']}\")\n",
        "    print(f\"Q2: {df_reset.iloc[X_train.shape[0] + i]['question2']}\")\n",
        "\n",
        "# Sample duplicates detected by Sentence-BERT model\n",
        "print(\"\\n\\n🔍 Sentence-BERT Predictions (Detected as Duplicates):\")\n",
        "sample_sbert = df[(df['sbert_pred'] == 1)].sample(n)\n",
        "for i, row in sample_sbert.iterrows():\n",
        "    print(f\"\\nQ1: {row['question1']}\")\n",
        "    print(f\"Q2: {row['question2']}\")\n",
        "    print(f\"SBERT Similarity: {row['sbert_sim']:.3f}\")\n"
      ],
      "metadata": {
        "id": "2wrezaFAfAdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir()) #Returns a list of files and folders in the current working directory\n"
      ],
      "metadata": {
        "id": "4aoL0Oq1VRcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q streamlit pyngrok nltk gensim joblib sentence-transformers\n"
      ],
      "metadata": {
        "id": "lXn6ORYhMW_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#====================================================\n",
        "# 🔹 Step 13: Streamlit App for Question Comparison\n",
        "# ====================================================\n",
        "\n",
        "!pip install gradio\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import joblib\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load models\n",
        "model_tfidf = joblib.load(\"model_tfidf.pkl\")\n",
        "tfidf_vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "model_w2v = joblib.load(\"model_w2v.pkl\")\n",
        "w2v_model = joblib.load(\"word2vec.model\")\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w not in stop_words and w not in string.punctuation]\n",
        "\n",
        "def avg_vector(tokens, model, dim=100):\n",
        "    valid_words = [w for w in tokens if w in model.wv]\n",
        "    return np.mean(model.wv[valid_words], axis=0) if valid_words else np.zeros(dim)\n",
        "\n",
        "def predict(q1, q2):\n",
        "    q1_tokens = preprocess(q1)\n",
        "    q2_tokens = preprocess(q2)\n",
        "    q1_clean = \" \".join(q1_tokens)\n",
        "    q2_clean = \" \".join(q2_tokens)\n",
        "\n",
        "    # TF-IDF\n",
        "    sim_tfidf = cosine_similarity(tfidf_vectorizer.transform([q1_clean]),\n",
        "                                  tfidf_vectorizer.transform([q2_clean]))[0][0]\n",
        "    pred_tfidf = model_tfidf.predict([[sim_tfidf]])[0]\n",
        "\n",
        "    # Word2Vec\n",
        "    vec1 = avg_vector(q1_tokens, w2v_model)\n",
        "    vec2 = avg_vector(q2_tokens, w2v_model)\n",
        "    pred_w2v = model_w2v.predict([np.abs(vec1 - vec2)])[0]\n",
        "\n",
        "    # SBERT\n",
        "    emb1 = sbert_model.encode([q1_clean])[0]\n",
        "    emb2 = sbert_model.encode([q2_clean])[0]\n",
        "    sim_sbert = cosine_similarity([emb1], [emb2])[0][0]\n",
        "    pred_sbert = int(sim_sbert >= 0.7)\n",
        "\n",
        "    return {\n",
        "        \"TF-IDF\": f\"{sim_tfidf:.3f} → {'Duplicate' if pred_tfidf else 'Not Duplicate'}\",\n",
        "        \"Word2Vec\": \"Duplicate\" if pred_w2v else \"Not Duplicate\",\n",
        "        \"SBERT\": f\"{sim_sbert:.3f} → {'Duplicate' if pred_sbert else 'Not Duplicate'}\"\n",
        "    }\n",
        "\n",
        "gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=[\"text\", \"text\"],\n",
        "    outputs=[\"text\", \"text\", \"text\"],\n",
        "    title=\"🔍 Quora Question Similarity Checker\",\n",
        "    description=\"Compare two questions using TF-IDF, Word2Vec, and SBERT models.\"\n",
        ").launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "bqmU1gXrQ40L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}