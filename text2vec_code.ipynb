{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF0NRo+OYvGBkYD6Vg0Vic",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fatima8024/NLP/blob/main/text2vec_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O42cvhiK2KK0",
        "outputId": "463ef2bb-57f6-41c0-8cc7-4a96066c2e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "UMRDw2H03nZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('COVID Fake News Data.csv')  # Replace with your file path\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Check class distribution\n",
        "print(df['outcome'].value_counts(normalize=True))\n",
        "\n",
        "# Split the dataset into training (70%) and temporary (30%) sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(\n",
        "    df['headlines'],\n",
        "    df['outcome'],\n",
        "    stratify=df['outcome'],  # Ensure the class distribution is consistent\n",
        "    test_size=0.3,\n",
        "    random_state=42  # For reproducibility\n",
        ")\n",
        "\n",
        "# Split the temporary set into validation (15%) and testing (15%) sets\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(\n",
        "    temp_text,\n",
        "    temp_labels,\n",
        "    stratify=temp_labels,\n",
        "    test_size=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Display the sizes of each set\n",
        "print(f\"Training size: {len(train_text)}\")\n",
        "print(f\"Validation size: {len(val_text)}\")\n",
        "print(f\"Testing size: {len(test_text)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44bZ8k4k4w5R",
        "outputId": "be2fddb5-ba72-4a8d-d09a-0875838cfb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           headlines  outcome\n",
            "0  A post claims compulsory vacination violates t...        0\n",
            "1  A photo claims that this person is a doctor wh...        0\n",
            "2  Post about a video claims that it is a protest...        0\n",
            "3  All deaths by respiratory failure and pneumoni...        0\n",
            "4  The dean of the College of Biologists of Euska...        0\n",
            "outcome\n",
            "0    0.953534\n",
            "1    0.046466\n",
            "Name: proportion, dtype: float64\n",
            "Training size: 7140\n",
            "Validation size: 1530\n",
            "Testing size: 1531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Fit and transform on training data\n",
        "train_tfidf = tfidf_vectorizer.fit_transform(train_text).toarray()\n",
        "\n",
        "# Transform validation and test data\n",
        "val_tfidf = tfidf_vectorizer.transform(val_text).toarray()\n",
        "test_tfidf = tfidf_vectorizer.transform(test_text).toarray()\n",
        "\n",
        "print(f\"Training TF-IDF shape: {train_tfidf.shape}\")\n",
        "print(f\"Validation TF-IDF shape: {val_tfidf.shape}\")\n",
        "print(f\"Testing TF-IDF shape: {test_tfidf.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnXXejrf6DoU",
        "outputId": "5d62bb55-32cc-48e2-a4f8-58450c864696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training TF-IDF shape: (7140, 5000)\n",
            "Validation TF-IDF shape: (1530, 5000)\n",
            "Testing TF-IDF shape: (1531, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert TF-IDF and labels to tensors\n",
        "train_seq = torch.tensor(train_tfidf, dtype=torch.float32)\n",
        "val_seq = torch.tensor(val_tfidf, dtype=torch.float32)\n",
        "test_seq = torch.tensor(test_tfidf, dtype=torch.float32)\n",
        "\n",
        "train_y = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "val_y = torch.tensor(val_labels.values, dtype=torch.long)\n",
        "test_y = torch.tensor(test_labels.values, dtype=torch.long)\n",
        "\n",
        "# Wrap tensors in TensorDatasets\n",
        "train_data = TensorDataset(train_seq, train_y)\n",
        "val_data = TensorDataset(val_seq, val_y)\n",
        "test_data = TensorDataset(test_seq, test_y)\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "-gIvszXE6MVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn # Import the torch.nn module\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.optim as optim # Import the optim module for optimizers\n",
        "import torch.nn.functional as F # Import torch.nn.functional\n",
        "\n",
        "\n",
        "\n",
        "class RNN_Text2Vec(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN_Text2Vec, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=5000, hidden_size=128, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 2)  # Output layer\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add a dimension for batch processing\n",
        "        rnn_out, _ = self.rnn(x)\n",
        "        rnn_out = rnn_out[:, -1, :]  # Take the last hidden state\n",
        "        logits = self.fc(rnn_out)\n",
        "        return self.softmax(logits)\n",
        "\n",
        "# Define the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, otherwise CPU\n",
        "\n",
        "# Initialize the model\n",
        "text2vec_rnn_model = RNN_Text2Vec().to(device)\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = optim.Adam(text2vec_rnn_model.parameters(), lr=1e-3) # Example using Adam optimizer\n"
      ],
      "metadata": {
        "id": "mAaMS7zA6WQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluate_text2vec function here\n",
        "def evaluate_text2vec():\n",
        "    text2vec_rnn_model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to calculate gradients during evaluation\n",
        "        for batch in val_dataloader:\n",
        "            batch = [t.to(device) for t in batch]\n",
        "            seq, labels = batch\n",
        "\n",
        "            preds = text2vec_rnn_model(seq)\n",
        "            loss = F.cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(preds, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    accuracy = total_correct / len(val_data)\n",
        "    return avg_loss, accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "bn4PLV_R-WXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_text2vec():\n",
        "    text2vec_rnn_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        seq, labels = batch\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        preds = text2vec_rnn_model(seq)  # Forward pass\n",
        "        loss = F.cross_entropy(preds, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(text2vec_rnn_model.parameters(), 1.0)  # Clip gradients\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "t4dk69Tr7BQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(10):  # Adjust epochs as needed\n",
        "    print(f'\\n Epoch {epoch + 1} / 10')\n",
        "    train_loss = train_text2vec()\n",
        "    valid_loss, _ = evaluate_text2vec()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(text2vec_rnn_model.state_dict(), 'best_text2vec_rnn_model.pt')\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muiNXmIt7E2m",
        "outputId": "e194731a-68b3-44c3-a020-5f73256ddd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.090\n",
            "Validation Loss: 0.109\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.042\n",
            "Validation Loss: 0.113\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.022\n",
            "Validation Loss: 0.122\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.013\n",
            "Validation Loss: 0.134\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.146\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.158\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.004\n",
            "Validation Loss: 0.166\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.178\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.185\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.002\n",
            "Validation Loss: 0.192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-28-584a0181526f\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "text2vec_rnn_model.load_state_dict(torch.load('best_text2vec_rnn_model.pt', weights_only=True))\n",
        "text2vec_rnn_model.eval()\n",
        "\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        seq, labels = batch\n",
        "        preds = text2vec_rnn_model(seq)\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "\n",
        "final_preds = np.argmax(all_preds, axis=1)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(classification_report(test_y, final_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t31j_CTG_WKV",
        "outputId": "934e9602-84cb-4669-ab30-69d0e762bacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98      1460\n",
            "           1       0.79      0.42      0.55        71\n",
            "\n",
            "    accuracy                           0.97      1531\n",
            "   macro avg       0.88      0.71      0.77      1531\n",
            "weighted avg       0.96      0.97      0.96      1531\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Text2Vec_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=5000, hidden_size=128, output_size=2):\n",
        "        super(Text2Vec_LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected output layer\n",
        "        self.softmax = nn.LogSoftmax(dim=1)  # Output probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add a batch dimension\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = lstm_out[:, -1, :]  # Take the last hidden state\n",
        "        logits = self.fc(lstm_out)\n",
        "        probs = self.softmax(logits)\n",
        "        return probs\n",
        "\n",
        "# Initialize the model\n",
        "text2vec_lstm_model = Text2Vec_LSTM(input_size=5000, hidden_size=128, output_size=2)\n",
        "text2vec_lstm_model = text2vec_lstm_model.to(device)  # Move to GPU if available\n"
      ],
      "metadata": {
        "id": "BdAh_Oz0Cik8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights based on class distribution in your training data\n",
        "# Assuming 'train_y' contains the labels for your training data\n",
        "class_counts = torch.bincount(train_y)  # Count occurrences of each class\n",
        "total_samples = len(train_y)\n",
        "weights = torch.tensor([total_samples / (2 * count) for count in class_counts], dtype=torch.float32)\n",
        "weights = weights.to(device)  # Move weights to the same device as your model\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(text2vec_lstm_model.parameters(), lr=1e-5)\n",
        "cross_entropy = nn.NLLLoss(weight=weights)\n"
      ],
      "metadata": {
        "id": "7v4iWop7CnPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_text2vec_lstm():\n",
        "    text2vec_lstm_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        seq, labels = batch\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        preds = text2vec_lstm_model(seq)  # Forward pass\n",
        "        loss = cross_entropy(preds, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(text2vec_lstm_model.parameters(), 1.0)  # Clip gradients\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "YdG29awADZZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_text2vec_lstm():\n",
        "    text2vec_lstm_model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            batch = [r.to(device) for r in batch]\n",
        "            seq, labels = batch\n",
        "            preds = text2vec_lstm_model(seq)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    return avg_loss, np.array(all_preds)\n"
      ],
      "metadata": {
        "id": "n_Mk2lrbDdbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(10):  # Adjust epochs as needed\n",
        "    print(f'\\n Epoch {epoch + 1} / 10')\n",
        "    train_loss = train_text2vec_lstm()\n",
        "    valid_loss, _ = evaluate_text2vec_lstm()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(text2vec_lstm_model.state_dict(), 'best_text2vec_lstm_model.pt')\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmkC48cfDg6J",
        "outputId": "44ede6eb-e0a1-4312-d06f-50f0a8343144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.690\n",
            "Validation Loss: 0.689\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.688\n",
            "Validation Loss: 0.687\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.687\n",
            "Validation Loss: 0.686\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.686\n",
            "Validation Loss: 0.685\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.684\n",
            "Validation Loss: 0.684\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.683\n",
            "Validation Loss: 0.683\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.680\n",
            "Validation Loss: 0.682\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.680\n",
            "Validation Loss: 0.681\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.679\n",
            "Validation Loss: 0.680\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    224.\n",
            "  Batch   100  of    224.\n",
            "  Batch   150  of    224.\n",
            "  Batch   200  of    224.\n",
            "\n",
            "Training Loss: 0.678\n",
            "Validation Loss: 0.679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2vec_lstm_model.load_state_dict(torch.load('best_text2vec_lstm_model.pt', weights_only=True))\n",
        "text2vec_lstm_model.eval()\n",
        "\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        seq, labels = batch\n",
        "        preds = text2vec_lstm_model(seq)\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "\n",
        "final_preds = np.argmax(all_preds, axis=1)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(classification_report(test_y, final_preds,zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpHFopGuEPru",
        "outputId": "1d7f2275-5deb-4743-f7b3-076bcb80e8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98      1460\n",
            "           1       0.00      0.00      0.00        71\n",
            "\n",
            "    accuracy                           0.95      1531\n",
            "   macro avg       0.48      0.50      0.49      1531\n",
            "weighted avg       0.91      0.95      0.93      1531\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "def compute_test_metrics(model, test_dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            batch = [r.to(device) for r in batch]\n",
        "            seq, labels = batch\n",
        "            preds = model(seq)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Collect predictions and labels\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            all_preds.extend(np.argmax(preds, axis=1))\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    avg_loss = total_loss / len(test_dataloader)\n",
        "\n",
        "    return test_accuracy, precision, recall, f1, avg_loss\n"
      ],
      "metadata": {
        "id": "rnqkEpPCG0Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect metrics for Text2Vec + RNN\n",
        "text2vec_rnn_metrics = compute_test_metrics(text2vec_rnn_model, test_dataloader)\n",
        "print(\"Text2Vec + RNN Metrics:\")\n",
        "print(f\"Accuracy: {text2vec_rnn_metrics[0]:.4f}\")\n",
        "print(f\"Precision: {text2vec_rnn_metrics[1]:.4f}\")\n",
        "print(f\"Recall: {text2vec_rnn_metrics[2]:.4f}\")\n",
        "print(f\"F1-Score: {text2vec_rnn_metrics[3]:.4f}\")\n",
        "print(f\"Loss Value: {text2vec_rnn_metrics[4]:.4f}\")\n",
        "\n",
        "# Collect metrics for Text2Vec + LSTM\n",
        "text2vec_lstm_metrics = compute_test_metrics(text2vec_lstm_model, test_dataloader)\n",
        "print(\"\\nText2Vec + LSTM Metrics:\")\n",
        "print(f\"Accuracy: {text2vec_lstm_metrics[0]:.4f}\")\n",
        "print(f\"Precision: {text2vec_lstm_metrics[1]:.4f}\")\n",
        "print(f\"Recall: {text2vec_lstm_metrics[2]:.4f}\")\n",
        "print(f\"F1-Score: {text2vec_lstm_metrics[3]:.4f}\")\n",
        "print(f\"Loss Value: {text2vec_lstm_metrics[4]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwIhLq4pG4wW",
        "outputId": "60c21c09-021d-4474-82e2-c7552fb56929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text2Vec + RNN Metrics:\n",
            "Accuracy: 0.9680\n",
            "Precision: 0.9640\n",
            "Recall: 0.9680\n",
            "F1-Score: 0.9633\n",
            "Loss Value: 0.7081\n",
            "\n",
            "Text2Vec + LSTM Metrics:\n",
            "Accuracy: 0.9536\n",
            "Precision: 0.9094\n",
            "Recall: 0.9536\n",
            "F1-Score: 0.9310\n",
            "Loss Value: 0.6801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "comparison_table = pd.DataFrame({\n",
        "    \"Metric\": [\"Test Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Loss Value\"],\n",
        "    \"Text2Vec + RNN\": text2vec_rnn_metrics,\n",
        "    \"Text2Vec + LSTM\": text2vec_lstm_metrics\n",
        "})\n",
        "\n",
        "# Display the table\n",
        "print(comparison_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIeE-UMlG-Gh",
        "outputId": "a9b70b88-c5ff-48b0-fdb0-50e1bf9181d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Metric  Text2Vec + RNN  Text2Vec + LSTM\n",
            "0  Test Accuracy        0.967995         0.953625\n",
            "1      Precision        0.964049         0.909401\n",
            "2         Recall        0.967995         0.953625\n",
            "3       F1-Score        0.963329         0.930988\n",
            "4     Loss Value        0.708111         0.680105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate the result in table format\n",
        "\n",
        "# ... (Your existing code)\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "comparison_table = pd.DataFrame({\n",
        "    \"Metric\": [\"Test Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Loss Value\"],\n",
        "    \"Text2Vec + RNN\": text2vec_rnn_metrics,\n",
        "    \"Text2Vec + LSTM\": text2vec_lstm_metrics\n",
        "})\n",
        "\n",
        "# Display the table with better formatting\n",
        "print(comparison_table.to_string(index=False)) # Use to_string for cleaner output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECP53jk2LDZv",
        "outputId": "1f24887e-8115-48fe-85e4-1a4a8b5fdba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Metric  Text2Vec + RNN  Text2Vec + LSTM\n",
            "Test Accuracy        0.967995         0.953625\n",
            "    Precision        0.964049         0.909401\n",
            "       Recall        0.967995         0.953625\n",
            "     F1-Score        0.963329         0.930988\n",
            "   Loss Value        0.708111         0.680105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: genrate a .csv file here\n",
        "\n",
        "# ... (Your existing code)\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "comparison_table = pd.DataFrame({\n",
        "    \"Metric\": [\"Test Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Loss Value\"],\n",
        "    \"Text2Vec + RNN\": text2vec_rnn_metrics,\n",
        "    \"Text2Vec + LSTM\": text2vec_lstm_metrics\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "comparison_table.to_csv('model_comparison.csv', index=False)"
      ],
      "metadata": {
        "id": "atyOOxytLWVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}