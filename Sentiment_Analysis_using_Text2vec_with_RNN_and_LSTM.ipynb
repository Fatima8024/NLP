{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwioC/6xZ7RcLq0agHtMQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fatima8024/NLP/blob/main/Sentiment_Analysis_using_Text2vec_with_RNN_and_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L2foO0ZDb_M"
      },
      "outputs": [],
      "source": [
        "!pip install transformers tensorflow scikit-learn pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "RH308VkZF_vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "# Load the dataset\n",
        "df = pd.read_csv('COVID Fake News Data.csv')  # Replace with your file name\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "IdLX1H8BDvKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check class distribution\n",
        "df['outcome'].value_counts(normalize = True)"
      ],
      "metadata": {
        "id": "qqIvE33JF2Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split train dataset into train, validation and test sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['headlines'], df['outcome'],\n",
        "                                                                    random_state=2018,\n",
        "                                                                    test_size=0.3,\n",
        "                                                                    stratify=df['outcome'])\n",
        "\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                                                                random_state=2018,\n",
        "                                                                test_size=0.5,\n",
        "                                                                stratify=temp_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "E8kKtQ3FHAxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "G4yjGrMkHRC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "metadata": {
        "id": "OAetR34NHbwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = None,\n",
        "    padding='max_length',\n",
        "    #pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = None,\n",
        "    padding='max_length',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = None,\n",
        "    padding='max_length',\n",
        "    truncation=True\n",
        ")"
      ],
      "metadata": {
        "id": "hfNKoAcvHjC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "\n",
        "# dataLoader for test set\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "0-erWLS7I1u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "3K8YWv6eKFZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "\n",
        "        # dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "        #pass the inputs to the model\n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "\n",
        "        x = self.fc1(cls_hs)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Rm2hgV9UKPgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "rQKs7oC9KZY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "claUmqcmip-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "import torch.optim as optim\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = optim.AdamW(model.parameters(),lr = 1e-5)\n",
        "\n",
        "#The compute_class_weight function from the sklearn.utils.class_weight module is used to compute the class weights with multiple parameters for the training labels.\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "print('Class Weights:',class_weights)"
      ],
      "metadata": {
        "id": "z-2R9pqbjGuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights)\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "gG-H18eWjAKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    # empty list to save model predictions\n",
        "    total_preds=[]\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "        # progress update after every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [r.to(device) for r in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        # clear previously calculated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # get model predictions for the current batch\n",
        "        preds = model(sent_id, mask)\n",
        "\n",
        "        # compute the loss between actual and predicted values\n",
        "        loss = cross_entropy(preds, labels)\n",
        "\n",
        "        # add on to the total loss\n",
        "        total_loss = total_loss + loss.item()\n",
        "\n",
        "        # backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # model predictions are stored on GPU. So, push it to CPU\n",
        "        preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "      # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    #returns the loss and predictions\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "qMLry60PmAqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "\n",
        "    print(\"\\nEvaluating...\")\n",
        "\n",
        "    # deactivate dropout layers\n",
        "    model.eval()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    # empty list to save the model predictions\n",
        "    total_preds = []\n",
        "\n",
        "    # Record the starting time before evaluation\n",
        "    t0 = time.time()\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "\n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,} Elapsed: {}'.format(step, len(val_dataloader), elapsed))\n",
        "          #  print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [t.to(device) for t in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        # deactivate autograd\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # model predictions\n",
        "            preds = model(sent_id, mask)\n",
        "\n",
        "            # compute the validation loss between actual and predicted values\n",
        "            loss = cross_entropy(preds,labels)\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    # compute the validation loss of the epoch\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "WTsql9TAmMKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "#defining epochs\n",
        "epochs = 10\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "PxiiJ-i3ofea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load weights of best modelpath = 'saved_weights.pt'model.load_state_dict(torch.load(path))\n",
        "\n",
        "# Load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path, weights_only=True))"
      ],
      "metadata": {
        "id": "nJZWbryZ2hfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data in batches\n",
        "batch_size = 32  # Adjust as needed\n",
        "preds_list = []\n",
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  for i in range(0, test_seq.shape[0], batch_size):\n",
        "        batch_seq = test_seq[i:i + batch_size].to(device)\n",
        "        batch_mask = test_mask[i:i + batch_size].to(device)\n",
        "        preds = model(batch_seq, batch_mask)\n",
        "        preds_list.extend(preds.detach().cpu().numpy())\n",
        "\n",
        "preds = np.array(preds_list)\n",
        "\n",
        "# get predictions for test data\n",
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "\n",
        "# Convert test_y to a NumPy array  <--- This is the added line\n",
        "test_y_np = test_y.cpu().numpy()  # Move to CPU and convert to NumPy\n",
        "\n",
        "print(classification_report(test_y, preds))\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions:\")\n",
        "for i, prediction in enumerate(preds):\n",
        "    print(f\"Sample {i}: {prediction}\")\n",
        "\n",
        "\n",
        "    #preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    #preds = preds.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "# model's performance\n",
        "#preds = np.argmax(preds, axis = 1)\n",
        "#print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kODzoJFF3nbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now for RNN using Chatgpt 4o"
      ],
      "metadata": {
        "id": "tvfn3KfNxVd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Update the Architecture for RNN\n",
        "\n",
        "class BERT_RNN(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_RNN, self).__init__()\n",
        "\n",
        "        self.bert = bert  # Pre-trained BERT model\n",
        "        self.rnn = nn.RNN(input_size=768, hidden_size=128, batch_first=True)  # Add RNN layer\n",
        "        self.fc = nn.Linear(128, 2)  # Fully connected output layer\n",
        "        self.softmax = nn.LogSoftmax(dim=1)  # Output probabilities\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        with torch.no_grad():  # Freeze BERT parameters\n",
        "            _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "\n",
        "        rnn_out, _ = self.rnn(cls_hs.unsqueeze(1))  # Pass embeddings through RNN\n",
        "        rnn_out = rnn_out[:, -1, :]  # Take the last output of the RNN\n",
        "        logits = self.fc(rnn_out)  # Fully connected layer\n",
        "        probs = self.softmax(logits)  # Softmax for probabilities\n",
        "        return probs\n",
        "\n",
        "# Create an instance of the model\n",
        "rnn_model = BERT_RNN(bert)\n",
        "rnn_model = rnn_model.to(device)  # Move to GPU if available\n"
      ],
      "metadata": {
        "id": "meMgIXT4wNem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Define Loss Function and Optimizer\n",
        "\n",
        "# Reuse the existing optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=1e-5)\n",
        "cross_entropy = nn.NLLLoss(weight=weights)\n"
      ],
      "metadata": {
        "id": "71EWXvDkx39n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Training Loop\n",
        "\n",
        "def train_rnn():\n",
        "    rnn_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        preds = rnn_model(sent_id, mask)  # Forward pass\n",
        "        loss = cross_entropy(preds, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 1.0)  # Clip gradients\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "iJ5LDBhrx7Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for Evaluation:\n",
        "\n",
        "\n",
        "def evaluate_rnn():\n",
        "    rnn_model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            batch = [r.to(device) for r in batch]\n",
        "            sent_id, mask, labels = batch\n",
        "            preds = rnn_model(sent_id, mask)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    return avg_loss, np.array(all_preds)\n"
      ],
      "metadata": {
        "id": "xwEeFdiCyALQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Training and Validation\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'\\n Epoch {epoch + 1} / {epochs}')\n",
        "    train_loss = train_rnn()\n",
        "    valid_loss, _ = evaluate_rnn()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(rnn_model.state_dict(), 'best_rnn_model.pt')\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n"
      ],
      "metadata": {
        "id": "TCn5MShQyDY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Testing the RNN\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "rnn_model.load_state_dict(torch.load('best_rnn_model.pt', map_location=device, weights_only=True))\n",
        "rnn_model.eval()\n",
        "\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        preds = rnn_model(sent_id, mask)\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "final_preds = np.argmax(all_preds, axis=1)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(classification_report(test_y, final_preds))\n"
      ],
      "metadata": {
        "id": "0ci-KG_uyFyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now applying LSTM"
      ],
      "metadata": {
        "id": "y1X2fHv2Rf58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_LSTM(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_LSTM, self).__init__()\n",
        "        self.bert = bert  # Pre-trained BERT model\n",
        "        self.lstm = nn.LSTM(input_size=768, hidden_size=128, batch_first=True, bidirectional=False)\n",
        "        self.fc = nn.Linear(128, 2)  # Fully connected output layer\n",
        "        self.softmax = nn.LogSoftmax(dim=1)  # Softmax for output probabilities\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        with torch.no_grad():  # Freeze BERT parameters\n",
        "            _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "\n",
        "        lstm_out, _ = self.lstm(cls_hs.unsqueeze(1))  # Pass embeddings through LSTM\n",
        "        lstm_out = lstm_out[:, -1, :]  # Take the last hidden state\n",
        "        logits = self.fc(lstm_out)\n",
        "        probs = self.softmax(logits)\n",
        "        return probs\n",
        "\n",
        "# Initialize the model\n",
        "lstm_model = BERT_LSTM(bert)\n",
        "lstm_model = lstm_model.to(device)  # Move to GPU if available\n"
      ],
      "metadata": {
        "id": "bbeZw_7JRcLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(lstm_model.parameters(), lr=1e-5)\n",
        "cross_entropy = nn.NLLLoss(weight=weights)\n"
      ],
      "metadata": {
        "id": "1xtGMzGvRmwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm():\n",
        "    lstm_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        preds = lstm_model(sent_id, mask)  # Forward pass\n",
        "        loss = cross_entropy(preds, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 1.0)  # Clip gradients\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "HwDQa-sqRpnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_lstm():\n",
        "    lstm_model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            batch = [r.to(device) for r in batch]\n",
        "            sent_id, mask, labels = batch\n",
        "            preds = lstm_model(sent_id, mask)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    return avg_loss, np.array(all_preds)\n"
      ],
      "metadata": {
        "id": "o7RSYbDBRtIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(10):  # Adjust epochs as needed\n",
        "    print(f'\\n Epoch {epoch + 1} / 10')\n",
        "    train_loss = train_lstm()\n",
        "    valid_loss, _ = evaluate_lstm()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(lstm_model.state_dict(), 'best_lstm_model.pt')\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n"
      ],
      "metadata": {
        "id": "Vp4SkF6VRwVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.load_state_dict(torch.load('best_lstm_model.pt'))\n",
        "lstm_model.eval()\n",
        "\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        preds = lstm_model(sent_id, mask)\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "\n",
        "final_preds = np.argmax(all_preds, axis=1)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(classification_report(test_y, final_preds))\n"
      ],
      "metadata": {
        "id": "Ar_Lr5rzR8SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def compute_metrics_from_report(model, test_dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    # Disable gradient computation\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            batch = [r.to(device) for r in batch]\n",
        "            #seq,labels = batch\n",
        "            sent_id,mask,labels = batch\n",
        "\n",
        "            # Model predictions\n",
        "            preds = model(sent_id,mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Extract predictions and true labels\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            all_preds.extend(np.argmax(preds, axis=1))\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "    precision = report['weighted avg']['precision']\n",
        "    recall = report['weighted avg']['recall']\n",
        "    f1 = report['weighted avg']['f1-score']\n",
        "    avg_loss = total_loss / len(test_dataloader)\n",
        "\n",
        "    return accuracy, precision, recall, f1, avg_loss\n",
        "\n",
        "# Extract metrics for Text2Vec + RNN\n",
        "rnn_metrics = compute_metrics_from_report(rnn_model, test_dataloader)\n",
        "print(\"Text2Vec + RNN Metrics:\")\n",
        "print(f\"Accuracy: {rnn_metrics[0]:.4f}\")\n",
        "print(f\"Precision: {rnn_metrics[1]:.4f}\")\n",
        "print(f\"Recall: {rnn_metrics[2]:.4f}\")\n",
        "print(f\"F1-Score: {rnn_metrics[3]:.4f}\")\n",
        "print(f\"Loss Value: {rnn_metrics[4]:.4f}\")\n",
        "\n",
        "# Extract metrics for Text2Vec + LSTM\n",
        "lstm_metrics = compute_metrics_from_report(lstm_model, test_dataloader)\n",
        "print(\"\\nText2Vec + LSTM Metrics:\")\n",
        "print(f\"Accuracy: {lstm_metrics[0]:.4f}\")\n",
        "print(f\"Precision: {lstm_metrics[1]:.4f}\")\n",
        "print(f\"Recall: {lstm_metrics[2]:.4f}\")\n",
        "print(f\"F1-Score: {lstm_metrics[3]:.4f}\")\n",
        "print(f\"Loss Value: {lstm_metrics[4]:.4f}\")\n",
        "\n",
        "# Create a table to compare results\n",
        "import pandas as pd\n",
        "\n",
        "comparison_table = pd.DataFrame({\n",
        "    \"Metric\": [\"Test Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Loss Value\"],\n",
        "    \"Text2Vec + RNN\":rnn_metrics,\n",
        "    \"Text2Vec + LSTM\":lstm_metrics\n",
        "})\n",
        "\n",
        "# Display the table\n",
        "print(\"\\nComparison Table:\")\n",
        "print(comparison_table)\n"
      ],
      "metadata": {
        "id": "hvwvDIxbShdY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}